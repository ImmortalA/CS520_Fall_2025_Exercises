\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{hyperref}

\geometry{margin=1in}

\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    showstringspaces=false,
    columns=flexible
}

\title{Exercise 2: Automated Testing \& Coverage}
\author{Anh Tran \\ CS520}

\begin{document}

\maketitle

\section{Part 1: Baseline Coverage Measurement}

\subsection{Overview}

This section presents baseline code coverage measurements for all 10 solutions generated in Exercise 1. The objective was to evaluate how well the existing test suites covered the code using \texttt{pytest-cov} to measure both line coverage and branch coverage. This establishes a baseline for identifying which problems might benefit from additional testing.

\subsection{Experimental Setup}

The coverage measurement setup consisted of:
\begin{itemize}
    \item \textbf{Tool}: pytest-cov with branch coverage enabled (\texttt{--cov-branch}) to measure branch coverage, which is the primary focus of improvement
    \item \textbf{Test Suite}: HumanEval+ benchmark tests from Exercise 1
    \item \textbf{Metrics}: Line coverage (\%), branch coverage (\%), and tests passed/total
    \item \textbf{Solutions}: All 10 problems from Exercise 1, using the Gemini SCoT solutions which demonstrated the best performance in Exercise 1
\end{itemize}

\subsection{Baseline Coverage Results}

Table \ref{tab:baseline_coverage} shows the complete baseline coverage measurements for all 10 problems:

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lccccl}
\toprule
\textbf{Problem} & \textbf{Function} & \textbf{Line \%} & \textbf{Branch \%} & \textbf{Tests} & \textbf{Interpretation} \\
\midrule
humaneval\_0 & has\_close\_elements & 100.0\% & 100.0\% & 1/1 & High branch coverage - most paths tested \\
humaneval\_1 & separate\_paren\_groups & 96.3\% & 90.0\% & 1/1 & High branch coverage - most paths tested \\
humaneval\_10 & make\_palindrome & 93.8\% & 75.0\% & 1/1 & Moderate branch coverage - some edge cases missing \\
humaneval\_12 & longest & 100.0\% & 100.0\% & 1/1 & High branch coverage - most paths tested \\
humaneval\_17 & parse\_music & 95.0\% & 87.5\% & 1/1 & High branch coverage - most paths tested \\
humaneval\_25 & factorize & 92.6\% & 90.0\% & 1/1 & High branch coverage - most paths tested \\
humaneval\_31 & is\_prime & 100.0\% & 100.0\% & 1/1 & High branch coverage - most paths tested \\
humaneval\_54 & same\_chars & 100.0\% & 0.0\% & 1/1 & Low branch coverage - no conditional branches \\
humaneval\_61 & correct\_bracketing & 94.4\% & 87.5\% & 1/1 & High branch coverage - most paths tested \\
humaneval\_108 & count\_nums & 100.0\% & 100.0\% & 1/1 & High branch coverage - most paths tested \\
\bottomrule
\end{tabular}
\caption{Baseline coverage measurements for all problems}
\label{tab:baseline_coverage}
\end{table}

\subsection{Key Observations}

Analysis of the results revealed several key observations:
\begin{itemize}
    \item \textbf{Average Line Coverage}: 97.2\% - indicating strong coverage across all problems, with each problem achieving at least 92.6\% line coverage, demonstrating that most code paths are being executed
    \item \textbf{Average Branch Coverage}: 82.9\% - lower than line coverage, which is expected. humaneval\_54 was excluded from this average as it contains no conditional branches
    \item \textbf{Special Case}: humaneval\_54 (\texttt{same\_chars}) showed 0\% branch coverage, which initially appeared problematic. However, analysis revealed the function implementation is simply \texttt{return set(s0) == set(s1)}, containing no conditional branches, loops, or exception handling. Therefore, 0\% branch coverage is the correct measurement for this function
    \item \textbf{Improvement Candidates}: humaneval\_54 and humaneval\_10 were selected based on the metric: $|\text{test\%} - \text{branch\%}| \times \text{test\%}$ to identify problems with maximum improvement potential
\end{itemize}

\subsection{Problem Selection for Part 2}

Two problems were selected for iterative test improvement:

\begin{enumerate}
    \item \textbf{humaneval\_54 (same\_chars)}: 0.0\% branch coverage - While this function contains no branches, additional tests could still validate correctness and detect bugs through comprehensive assertion testing
    \item \textbf{humaneval\_10 (make\_palindrome)}: 75.0\% branch coverage - Selected as a candidate with clear room for improvement through enhanced edge case testing
\end{enumerate}

These problems were chosen based on maximum improvement potential, determined by analyzing the gap between test coverage and branch coverage.

\newpage
\section{Part 2: LLM-Assisted Test Generation \& Coverage Improvement}

\subsection{Overview}

Part 2 focuses on improving test coverage through iterative test generation. The approach involves generating additional tests iteratively until coverage improvement converges. The two problems selected from Part 1, humaneval\_54 and humaneval\_10, serve as the focus for this improvement process.

\subsection{Experimental Setup}

The experimental setup consisted of:
\begin{itemize}
    \item \textbf{Selected Problems}: humaneval\_54 (same\_chars), humaneval\_10 (make\_palindrome)
    \item \textbf{Test Generation}: Gemini 2.5 Flash API was used, with a fallback to mock tests when the API was unavailable
    \item \textbf{Strategy}: Iterative test generation where each iteration generates new tests, measures coverage, and determines whether to continue
    \item \textbf{Convergence Criteria}: Stopping condition was 3 consecutive iterations with less than 3\% increase in branch coverage
    \item \textbf{Max Iterations}: A limit of 10 iterations was set, though both problems converged after 5 iterations
\end{itemize}

\subsection{Prompt Design}

A prompt was designed to guide test generation. The prompt template used:

\begin{lstlisting}[caption={LLM Test Generation Prompt}]
You are an expert at writing comprehensive unit tests for Python functions.

Current situation:
- Function: {function_name}
- Function description: {description}
- Current line coverage: {current_line}%
- Current branch coverage: {current_branch}%

Existing tests:
{existing_tests}

Your task:
Generate additional unit tests that will increase branch coverage. Focus on:
1. Testing edge cases and boundary conditions
2. Testing different code paths and branches
3. Testing error conditions if applicable
4. Testing various input combinations

Output format: Python code with test functions following this structure:

def check_additional(candidate):
    # Your new test cases here
    # Use assertion(out, exp, atol) helper if needed
    pass

def run_additional_tests(func):
    try:
        check_additional(func)
        return (1, 1)
    except AssertionError:
        return (0, 1)
    except Exception:
        return (0, 1)

Generate ONLY the Python code, no explanations. Make sure tests are non-redundant with existing tests.
\end{lstlisting}

\subsection{Results: humaneval\_54 (same\_chars)}

\subsubsection{Iteration Summary}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{cccc}
\toprule
\textbf{Iteration} & \textbf{Line \%} & \textbf{Branch \%} & \textbf{Improvement} \\
\midrule
Baseline & 100.0\% & 0.0\% & -- \\
1 & 100.0\% & 0.0\% & +0.0\% \\
2 & 100.0\% & 0.0\% & +0.0\% \\
3 & 100.0\% & 0.0\% & +0.0\% \\
4 & 100.0\% & 0.0\% & +0.0\% \\
5 & 100.0\% & 0.0\% & +0.0\% \\
\bottomrule
\end{tabular}
\caption{Test improvement iterations for humaneval\_54}
\label{tab:humaneval_54_iterations}
\end{table}

\subsubsection{Analysis}

\textbf{Why Branch Coverage Remains 0\%:}

Initial analysis showed branch coverage remained at 0\% despite extensive test additions. Examination of the function implementation revealed:

\begin{lstlisting}
def same_chars(s0: str, s1: str) -> bool:
    return set(s0) == set(s1)
\end{lstlisting}

This function contains \textbf{no conditional branches} - no if/else statements, loops, or exception handling. It consists of a single expression. Therefore, branch coverage cannot improve beyond 0\% regardless of the number of tests added. This observation highlights that branch coverage is not always an appropriate metric for all functions.

\textbf{Test Generation:}

Despite no branch coverage improvement, the test generation process produced a substantial number of tests across 5 iterations:
\begin{itemize}
    \item Iteration 1: 15 test cases covering basic scenarios including empty strings, single characters, case sensitivity, and special characters
    \item Iteration 2: 28 test cases expanding to additional edge cases
    \item Iteration 3: 50 test cases incorporating unicode and mixed data types
    \item Iteration 4: 73 test cases providing comprehensive coverage
    \item Iteration 5: 73+ test cases indicating convergence
\end{itemize}

\textbf{Test Quality:}

The generated tests demonstrated substantial variety and comprehensiveness, covering:
\begin{itemize}
    \item Empty string combinations
    \item Single character cases
    \item Case sensitivity validation
    \item Special characters and whitespace
    \item Unicode character handling
    \item Mixed alphanumeric inputs
    \item Long strings with various patterns
\end{itemize}

Although these tests did not improve branch coverage (due to the absence of branches), they remain valuable for validating correctness, as demonstrated in Part 3.

\subsection{Results: humaneval\_10 (make\_palindrome)}

\subsubsection{Iteration Summary}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{cccc}
\toprule
\textbf{Iteration} & \textbf{Line \%} & \textbf{Branch \%} & \textbf{Improvement} \\
\midrule
Baseline & 93.8\% & 75.0\% & -- \\
1 & 93.8\% & 75.0\% & +0.0\% \\
2 & 93.8\% & 75.0\% & +0.0\% \\
3 & 93.8\% & 75.0\% & +0.0\% \\
4 & 93.8\% & 75.0\% & +0.0\% \\
5 & 93.8\% & 75.0\% & +0.0\% \\
\bottomrule
\end{tabular}
\caption{Test improvement iterations for humaneval\_10}
\label{tab:humaneval_10_iterations}
\end{table}

\subsubsection{Analysis}

\textbf{Coverage Plateau:}

A notable observation was that branch coverage remained constant at 75.0\% across all 5 iterations despite generating additional tests. Analysis suggests this occurred because:
\begin{itemize}
    \item The existing tests already cover the main code paths effectively
    \item The remaining 25\% of branches likely require very specific edge cases that are difficult to exercise
    \item The generated tests may not have targeted the correct uncovered branches
    \item Some branches may be difficult to exercise, such as error paths or rare conditions that do not occur in normal testing scenarios
\end{itemize}

\textbf{Test Generation:}

Despite no coverage improvement, the test generation process continued to produce additional tests each iteration:
\begin{itemize}
    \item Iteration 1: 10 test cases focusing on palindrome detection and suffix handling
    \item Iteration 2: 35 test cases expanding with additional palindrome cases
    \item Iteration 3: 46 test cases incorporating edge cases and special characters
    \item Iteration 4: 62 test cases providing comprehensive scenarios
    \item Iteration 5: 62+ test cases indicating convergence
\end{itemize}

While a substantial number of tests were generated, they did not exercise the uncovered branches. This indicates that simply generating more tests is insufficient - targeted testing of specific uncovered paths is necessary.

\textbf{Test Examples Generated:}

\begin{lstlisting}
# Already palindrome cases
assert candidate('a') == 'a'
assert candidate('aba') == 'aba'
assert candidate('racecar') == 'racecar'

# Single character suffix (most common case)
assert candidate('abc') == 'abcba'
assert candidate('abcdef') == 'abcdefedcba'

# Multi-character palindromic suffix
assert candidate('abcc') == 'abccba'  # 'cc' is suffix
assert candidate('banana') == 'bananab'  # 'anana' is suffix
\end{lstlisting}

\subsection{Redundancy Analysis}

\textbf{Test Deduplication:}

The test merging process was implemented to ensure:
\begin{itemize}
    \item Original benchmark tests are preserved to maintain baseline test coverage
    \item Additional tests are appended (not replacing) to allow test accumulation
    \item The \texttt{run\_tests} wrapper combines both test suites for unified execution
    \item Tests are executed cumulatively across iterations, with each iteration building upon previous ones
\end{itemize}

\textbf{Redundancy Observations:}

Several redundancy issues were observed:
\begin{itemize}
    \item \textbf{Some redundancy observed}: Similar test cases were generated across iterations, such as repeated empty string tests
    \item \textbf{No explicit deduplication}: Deduplication was not implemented, so tests accumulated as-is. While pytest handles this adequately, it is not optimal
    \item \textbf{Impact}: Minimal - redundant tests do not affect coverage measurement, only execution time
    \item \textbf{Future improvement}: Test case normalization could be added to detect and skip near-duplicate assertions
\end{itemize}

\subsection{Part 2 Conclusion}

Part 2 yielded several important findings:
\begin{itemize}
    \item \textbf{humaneval\_54}: Branch coverage cannot improve due to the absence of branches, but a comprehensive test suite was generated that proved valuable in subsequent analysis
    \item \textbf{humaneval\_10}: Branch coverage plateaued at 75\%, indicating that the remaining branches are difficult to exercise through standard test generation
    \item \textbf{Test Generation Effectiveness}: The process generated diverse, comprehensive test cases, though they did not improve measured branch coverage. However, these tests demonstrated value in fault detection
    \item \textbf{Test Quality}: The generated tests cover edge cases, boundary conditions, and various input combinations effectively, though they did not target the specific uncovered branches
\end{itemize}

While coverage metrics did not improve as initially expected, this experience highlighted that coverage metrics alone do not fully capture test suite quality. The generated tests demonstrated utility in fault detection, as shown in Part 3.

\newpage
\section{Part 3: Fault Detection Check}

\subsection{Overview}

Part 3 evaluates whether the test suites generated in Part 2 are effective at fault detection. Despite not improving coverage metrics, these tests may still detect bugs. To assess this, realistic bugs were injected into the original solutions and the improved test suites were executed to determine detection capability.

\subsection{Experimental Setup}

The fault detection experiment was configured as follows:
\begin{itemize}
    \item \textbf{Problems Tested}: The same two problems from Part 2 (humaneval\_54, humaneval\_10)
    \item \textbf{Test Suite}: Improved tests from Part 2 (final iteration with all accumulated tests)
    \item \textbf{Bug Injection}: Realistic bugs representative of common programming errors were injected
    \item \textbf{Evaluation}: Tests were executed against buggy code to determine whether failures occurred, indicating effective fault detection
\end{itemize}

\subsection{Bug Injection Strategy}

Bugs were injected to represent realistic programming errors:

\begin{itemize}
    \item \textbf{For same\_chars}: Changed \texttt{==} to \texttt{!=}, inverting all results. This represents a common typo that could easily occur
    \item \textbf{For make\_palindrome}: Changed \texttt{range(n + 1)} to \texttt{range(n)}, a classic off-by-one error that misses the empty string case
\end{itemize}

These bugs were selected because they:
\begin{enumerate}
    \item Represent common programming errors - wrong operator and off-by-one errors are frequently encountered
    \item Are subtle enough to pass some tests while failing others, rather than being obviously broken
    \item Exercise different code paths than the original implementation, testing whether the test suite can detect real problems
\end{enumerate}

\subsection{Results}

\subsubsection{humaneval\_54 (same\_chars)}

\textbf{Injected Bug:}
\begin{lstlisting}
# Original:
return set(s0) == set(s1)

# Buggy:
return set(s0) != set(s1)  # Wrong operator: inverts result
\end{lstlisting}

\textbf{Result:} \textbf{SUCCESS - Bug was caught by tests}

\textbf{Analysis:}
The test suite successfully detected the injected bug. Analysis reveals:
\begin{itemize}
    \item The bug inverts all return values - True becomes False and False becomes True
    \item The comprehensive test suite generated in Part 2 contains numerous assertions that immediately fail
    \item For example, tests such as \texttt{assert candidate("abca", "baca") is True} fail because the buggy function returns False instead
    \item \textbf{Conclusion}: Despite not improving branch coverage (due to the absence of branches), the tests successfully detected the bug, demonstrating their value for fault detection
\end{itemize}

\subsubsection{humaneval\_10 (make\_palindrome)}

\textbf{Injected Bug:}
\begin{lstlisting}
# Original:
for i in range(n + 1):  # Includes empty string case (i=n)

# Buggy:
for i in range(n):  # Off-by-one: misses empty string case
\end{lstlisting}

\textbf{Result:} \textbf{SUCCESS - Bug was caught by tests}

\textbf{Analysis:}
The test suite successfully detected this bug as well. Analysis shows:
\begin{itemize}
    \item The bug prevents the loop from checking the empty string case (when \texttt{i = n})
    \item This affects edge cases, particularly empty input strings
    \item The improved test suite includes edge case tests that exercise this path
    \item \textbf{Conclusion}: Despite branch coverage remaining at 75\%, the tests successfully detected the off-by-one error, demonstrating that comprehensive test cases are more important than coverage percentages alone
\end{itemize}

\subsection{Fault Detection Summary}

\begin{table}[h]
\centering
\begin{tabular}{lcccl}
\toprule
\textbf{Problem} & \textbf{Bug Type} & \textbf{Caught?} & \textbf{Test Suite} & \textbf{Conclusion} \\
\midrule
humaneval\_54 & Operator inversion (\texttt{==} $\to$ \texttt{!=}) & Yes & Improved (Part 2) & Tests successfully detected bug \\
humaneval\_10 & Off-by-one (\texttt{range(n+1)} $\to$ \texttt{range(n)}) & Yes & Improved (Part 2) & Tests successfully detected bug \\
\bottomrule
\end{tabular}
\caption{Fault detection results}
\label{tab:fault_detection}
\end{table}

\subsection{Coverage vs. Fault Detection Analysis}

\textbf{Key Finding:}

This analysis revealed a significant finding: despite branch coverage not improving in Part 2, the \textbf{improved test suites successfully caught all injected bugs}. This demonstrates several important principles:

\begin{enumerate}
    \item \textbf{Coverage is not the only metric}: High coverage does not guarantee bug detection, but comprehensive test cases do. The initial focus on coverage numbers may have been misplaced
    \item \textbf{Test diversity matters}: The variety of test cases (edge cases, boundary conditions) caught bugs that might be missed by coverage-focused tests
    \item \textbf{For same\_chars}: Even with 0\% branch coverage (no branches exist), the test suite validated correctness and caught the operator bug, demonstrating that coverage metrics can be misleading
    \item \textbf{For make\_palindrome}: The 75\% branch coverage was sufficient to catch the off-by-one error through edge case testing, validating the utility of the generated tests
\end{enumerate}

\textbf{Linking Coverage to Fault Detection:}

The relationship between coverage and fault detection reveals important insights:
\begin{itemize}
    \item \textbf{humaneval\_54}: 0\% branch coverage (no branches) $\rightarrow$ Tests still caught the bug through assertion validation, indicating that coverage metrics can be misleading
    \item \textbf{humaneval\_10}: 75\% branch coverage $\rightarrow$ Despite some branches remaining uncovered, the edge case tests successfully detected the off-by-one bug
    \item \textbf{Insight}: Test comprehensiveness (variety of inputs) is as important as coverage percentage for fault detection, and may be more important in practice
\end{itemize}

\subsection{Part 3 Conclusion}

Both injected bugs were successfully detected by the improved test suites:
\begin{itemize}
    \item \textbf{humaneval\_54}: Operator inversion bug caught through comprehensive assertion testing, despite 0\% branch coverage
    \item \textbf{humaneval\_10}: Off-by-one error caught through edge case testing, validating the utility of the generated tests
    \item \textbf{Success Rate}: 100\% (2/2 bugs detected)
    \item \textbf{Key Insight}: Comprehensive test cases covering diverse inputs and edge cases are effective at fault detection, even when branch coverage metrics do not show improvement. This suggests that focusing solely on coverage percentages may be less valuable than ensuring test comprehensiveness
\end{itemize}

\newpage
\section{Overall Conclusion}

\subsection{Summary}

Summary of the complete exercise:
\begin{itemize}
    \item \textbf{Part 1}: Baseline coverage was measured for 10 problems, achieving average 97.2\% line coverage and 82.9\% branch coverage (excluding the no-branch case), establishing a strong foundation
    \item \textbf{Part 2}: Comprehensive test suites were generated for 2 selected problems through 5 iterations - while coverage metrics did not improve, test diversity increased significantly
    \item \textbf{Part 3}: Successfully detected 100\% of injected bugs (2/2) using the improved test suites, validating the work performed in Part 2
\end{itemize}

\subsection{Key Insights}

The exercise yielded several important insights:

\begin{enumerate}
    \item \textbf{Branch Coverage Limitations}: Some functions (like \texttt{same\_chars}) have no branches, making branch coverage an inappropriate metric. Line coverage and test comprehensiveness are more relevant measures.
    
    \item \textbf{Coverage vs. Fault Detection}: High coverage percentages do not guarantee bug detection. Comprehensive test cases covering diverse inputs and edge cases are significantly more effective than focusing solely on coverage numbers.
    
    \item \textbf{Test Generation}: Automated test generation can produce diverse, comprehensive test cases, though it may not always target uncovered branches effectively. However, the generated tests remain valuable for validation even when coverage does not improve, as demonstrated in Part 3.
    
    \item \textbf{Test Accumulation}: Iterative test generation successfully accumulates tests across iterations, building comprehensive test suites that improve fault detection capability.
    
    \item \textbf{Function-Specific Bug Injection}: Targeted bug injection based on function structure (operator changes, off-by-one errors) creates realistic, testable bugs that validate test suite effectiveness.
\end{enumerate}

\subsection{Lessons Learned}

Lessons learned regarding what worked and what did not:
\begin{itemize}
    \item \textbf{What worked}: Function-specific bug injection strategies created realistic, detectable bugs that effectively tested the test suites
    \item \textbf{What worked}: Comprehensive test case generation improved fault detection significantly, even without coverage improvement
    \item \textbf{What did not work as well}: Branch coverage did not improve even with extensive test generation, indicating that remaining branches are difficult to exercise through standard test generation approaches
    \item \textbf{Future Work}: Mutation testing or property-based testing could be explored to better target uncovered branches, potentially proving more effective than simply generating more tests
\end{itemize}

Overall, this exercise demonstrated that coverage metrics are useful but not sufficient. Test cases that cover diverse scenarios are more important than achieving a specific coverage percentage. Part 3 validated that the work performed in Part 2 was valuable, despite coverage numbers not improving.

\section{Repository}

All code, test files, coverage reports, and results:\\
\url{https://github.com/ImmortalA/CS520_Fall_2025_Exercises}

\begin{thebibliography}{9}
\bibitem{humanevalplus}
EvalPlus Team. HumanEval+ Dataset. 
\url{https://huggingface.co/datasets/evalplus/humanevalplus}, 2023.

\bibitem{pytest-cov}
pytest-cov Documentation.
\url{https://pytest-cov.readthedocs.io/}, 2024.
\end{thebibliography}

\end{document}

